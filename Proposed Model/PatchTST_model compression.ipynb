{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b51a5131-53db-4014-9c5c-bb707adce61f",
      "metadata": {
        "id": "b51a5131-53db-4014-9c5c-bb707adce61f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch.nn as nn  # nn 모듈 임포트 추가\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28a6e3b0-ef1e-43ec-83d4-54c01e97be29",
      "metadata": {
        "id": "28a6e3b0-ef1e-43ec-83d4-54c01e97be29"
      },
      "outputs": [],
      "source": [
        "df_ETTh1 = pd.read_csv(\"./ETTh1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "86ddea18-9509-42c9-91e0-bf1d5d1d0dc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "86ddea18-9509-42c9-91e0-bf1d5d1d0dc1",
        "outputId": "2ee6eaf3-69d9-4fbc-fe3f-a3c020c024a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
              "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
              "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
              "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
              "3  2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
              "4  2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d9bf6a1-01e9-47f2-a2d7-aa5e4b0ecfbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>HUFL</th>\n",
              "      <th>HULL</th>\n",
              "      <th>MUFL</th>\n",
              "      <th>MULL</th>\n",
              "      <th>LUFL</th>\n",
              "      <th>LULL</th>\n",
              "      <th>OT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01 00:00:00</td>\n",
              "      <td>5.827</td>\n",
              "      <td>2.009</td>\n",
              "      <td>1.599</td>\n",
              "      <td>0.462</td>\n",
              "      <td>4.203</td>\n",
              "      <td>1.340</td>\n",
              "      <td>30.531000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01 01:00:00</td>\n",
              "      <td>5.693</td>\n",
              "      <td>2.076</td>\n",
              "      <td>1.492</td>\n",
              "      <td>0.426</td>\n",
              "      <td>4.142</td>\n",
              "      <td>1.371</td>\n",
              "      <td>27.787001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-01 02:00:00</td>\n",
              "      <td>5.157</td>\n",
              "      <td>1.741</td>\n",
              "      <td>1.279</td>\n",
              "      <td>0.355</td>\n",
              "      <td>3.777</td>\n",
              "      <td>1.218</td>\n",
              "      <td>27.787001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-01 03:00:00</td>\n",
              "      <td>5.090</td>\n",
              "      <td>1.942</td>\n",
              "      <td>1.279</td>\n",
              "      <td>0.391</td>\n",
              "      <td>3.807</td>\n",
              "      <td>1.279</td>\n",
              "      <td>25.044001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-01 04:00:00</td>\n",
              "      <td>5.358</td>\n",
              "      <td>1.942</td>\n",
              "      <td>1.492</td>\n",
              "      <td>0.462</td>\n",
              "      <td>3.868</td>\n",
              "      <td>1.279</td>\n",
              "      <td>21.948000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d9bf6a1-01e9-47f2-a2d7-aa5e4b0ecfbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d9bf6a1-01e9-47f2-a2d7-aa5e4b0ecfbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d9bf6a1-01e9-47f2-a2d7-aa5e4b0ecfbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-53a08a72-2872-4948-84cd-da8cc5092729\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53a08a72-2872-4948-84cd-da8cc5092729')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-53a08a72-2872-4948-84cd-da8cc5092729 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_ETTh1",
              "summary": "{\n  \"name\": \"df_ETTh1\",\n  \"rows\": 17420,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 17420,\n        \"samples\": [\n          \"2017-11-26 20:00:00\",\n          \"2016-07-01 19:00:00\",\n          \"2016-09-10 06:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HUFL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.067743958957745,\n        \"min\": -22.70599937438965,\n        \"max\": 23.643999099731445,\n        \"num_unique_values\": 613,\n        \"samples\": [\n          -8.23900032043457,\n          9.041999816894531,\n          7.099999904632568\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HULL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.042342307036328,\n        \"min\": -4.75600004196167,\n        \"max\": 10.11400032043457,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          0.6029999852180481,\n          3.2149999141693115,\n          5.090000152587892\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MUFL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.826978212457667,\n        \"min\": -25.08799934387207,\n        \"max\": 17.340999603271484,\n        \"num_unique_values\": 1031,\n        \"samples\": [\n          6.928999900817871,\n          10.838000297546388,\n          8.635000228881836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MULL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8092934715308207,\n        \"min\": -5.934000015258789,\n        \"max\": 7.747000217437744,\n        \"num_unique_values\": 314,\n        \"samples\": [\n          -1.4919999837875366,\n          2.203000068664551,\n          -2.45199990272522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LUFL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1645059523875725,\n        \"min\": -1.187999963760376,\n        \"max\": 8.498000144958496,\n        \"num_unique_values\": 236,\n        \"samples\": [\n          5.6040000915527335,\n          1.4320000410079956,\n          6.335000038146973\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LULL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5995518293119606,\n        \"min\": -1.371000051498413,\n        \"max\": 3.0460000038146973,\n        \"num_unique_values\": 126,\n        \"samples\": [\n          -0.3350000083446503,\n          1.9490000009536743,\n          -0.0299999993294477\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"OT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.566946318051215,\n        \"min\": -4.079999923706056,\n        \"max\": 46.00699996948242,\n        \"num_unique_values\": 669,\n        \"samples\": [\n          18.92300033569336,\n          36.018001556396484,\n          11.1850004196167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df_ETTh1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3629fff9-ebdd-4366-9d98-869d494c5f96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3629fff9-ebdd-4366-9d98-869d494c5f96",
        "outputId": "ff081033-4e57-4e10-d265-1c90c841f629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17420, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_ETTh1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "37252b8f-be86-4d2f-8265-5ae2f656525f",
      "metadata": {
        "id": "37252b8f-be86-4d2f-8265-5ae2f656525f"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 정의 및 처리\n",
        "\n",
        "class ETTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset=\"ETTh1\", mode=\"train\", scale=True, seq_len=336, pred_len=96):\n",
        "        super().__init__()\n",
        "        file_path = f\"./{dataset}.csv\"  # 현재 디렉토리에 파일 이름 그대로 사용\n",
        "        df = pd.read_csv(file_path)\n",
        "        x_y = df.iloc[:,1:]\n",
        "        time_stamp = df.iloc[:,0]\n",
        "\n",
        "        assert mode in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[mode]\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "\n",
        "        # 시퀀스 경계 설정\n",
        "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        # 데이터 정규화\n",
        "        if scale:\n",
        "            train_x_y = x_y.iloc[border1s[0]: border2s[0]]\n",
        "            self.ss = StandardScaler()\n",
        "            self.ss.fit(train_x_y.to_numpy(dtype=np.float32))\n",
        "            x_y = self.ss.transform(x_y.to_numpy(dtype=np.float32))\n",
        "        else:\n",
        "            x_y = x_y.to_numpy(dtype=np.float32)\n",
        "\n",
        "        time_stamp = time_stamp.to_numpy()\n",
        "\n",
        "        self.data_x = x_y[border1: border2, :]\n",
        "        self.data_y = x_y[border1: border2, -1]\n",
        "\n",
        "        self.data_stamp = time_stamp[border1: border2]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end\n",
        "        r_end = r_begin + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        return seq_x, seq_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.ss.inverse_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "865f87cd-958b-47af-9b95-80db01f90fb6",
      "metadata": {
        "id": "865f87cd-958b-47af-9b95-80db01f90fb6"
      },
      "outputs": [],
      "source": [
        "# 데이터 정규화 실행 및 Positional Encoding (위치 인코딩)\n",
        "\n",
        "class RevIN(torch.nn.Module):\n",
        "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False, target_idx=-1):\n",
        "        super(RevIN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        self.target_idx = target_idx\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode):\n",
        "        if mode == \"norm\":\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == \"denorm\":\n",
        "            x = self._denormalize(x)\n",
        "        else: raise AssertionError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.affine_weight = torch.nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = torch.nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim-1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:,-1,:].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps*self.eps)\n",
        "        x = x * self.stdev[:, :, self.target_idx]\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last[:, :, self.target_idx]\n",
        "        else:\n",
        "            x = x + self.mean[:, :, self.target_idx]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transpose(torch.nn.Module):\n",
        "    def __init__(self, *dims, contiguous=False):\n",
        "        super().__init__()\n",
        "        self.dims, self.contiguous = dims, contiguous\n",
        "    def forward(self, x):\n",
        "        if self.contiguous:\n",
        "            return x.transpose(*self.dims).contiguous()\n",
        "        else:\n",
        "            return x.transpose(*self.dims)\n",
        "\n",
        "\n",
        "def positional_encoding(q_len, d_model):\n",
        "    W_pos = torch.empty((q_len, d_model))\n",
        "    torch.nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    return torch.nn.Parameter(W_pos, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5f3fe30-75cd-43e5-a4b6-257edd8a7645",
      "metadata": {
        "id": "d5f3fe30-75cd-43e5-a4b6-257edd8a7645"
      },
      "outputs": [],
      "source": [
        "# PatchTST Encorder 레이어 정의\n",
        "\n",
        "class TSTiEncoder(torch.nn.Module):  #i means channel-independent\n",
        "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., store_attn=False,\n",
        "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
        "                 verbose=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "        # Input encoding\n",
        "        q_len = patch_num\n",
        "        self.W_P = torch.nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
        "        self.seq_len = q_len\n",
        "        # Positional encoding\n",
        "        self.W_pos = positional_encoding(q_len, d_model)\n",
        "        # Residual dropout\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        # Encoder\n",
        "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                   pre_norm=pre_norm, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
        "\n",
        "    def forward(self, x):                                              # x: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "        # Input encoding\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
        "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
        "\n",
        "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
        "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
        "\n",
        "        # Encoder\n",
        "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
        "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
        "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "# Cell\n",
        "class TSTEncoder(torch.nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None,\n",
        "                        norm='BatchNorm', attn_dropout=0., dropout=0.,\n",
        "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
        "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                                      res_attention=res_attention,\n",
        "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
        "        self.res_attention = res_attention\n",
        "\n",
        "    def forward(self, src, key_padding_mask=None, attn_mask=None):\n",
        "        output = src\n",
        "        scores = None\n",
        "        if self.res_attention:\n",
        "            for mod in self.layers:\n",
        "                output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "        else:\n",
        "            for mod in self.layers:\n",
        "                output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "class TSTEncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
        "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, res_attention=False, pre_norm=False):\n",
        "        super().__init__()\n",
        "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        # Multi-Head attention\n",
        "        self.res_attention = res_attention\n",
        "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_attn = torch.nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_attn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_attn = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed-Forward\n",
        "        self.ff = torch.nn.Sequential(torch.nn.Linear(d_model, d_ff, bias=bias),\n",
        "                                torch.nn.GELU(),\n",
        "                                torch.nn.Dropout(dropout),\n",
        "                                torch.nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_ffn = torch.nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_ffn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_ffn = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.store_attn = store_attn\n",
        "\n",
        "\n",
        "    def forward(self, src, prev=None, key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "        # Multi-Head attention sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "        ## Multi-Head attention\n",
        "        if self.res_attention:\n",
        "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        if self.store_attn:\n",
        "            self.attn = attn\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "        ## Position-wise Feed-Forward\n",
        "        src2 = self.ff(src)\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return src, scores\n",
        "        else:\n",
        "            return src\n",
        "\n",
        "\n",
        "class _MultiheadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
        "        super().__init__()\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "        self.W_Q = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_K = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_V = torch.nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "        # Scaled Dot-Product Attention (multiple heads)\n",
        "        self.res_attention = res_attention\n",
        "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
        "\n",
        "        # Poject output\n",
        "        self.to_out = torch.nn.Sequential(torch.nn.Linear(n_heads * d_v, d_model), torch.nn.Dropout(proj_dropout))\n",
        "\n",
        "    def forward(self, Q, K=None, V=None, prev=None,\n",
        "                key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "        bs = Q.size(0)\n",
        "        if K is None: K = Q\n",
        "        if V is None: V = Q\n",
        "\n",
        "        # Linear (+ split in multiple heads)\n",
        "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
        "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
        "\n",
        "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "        if self.res_attention:\n",
        "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # back to the original inputs dimensions\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
        "        output = self.to_out(output)\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights\n",
        "\n",
        "\n",
        "class _ScaledDotProductAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = torch.nn.Dropout(attn_dropout)\n",
        "        self.res_attention = res_attention\n",
        "        head_dim = d_model // n_heads\n",
        "        self.scale = torch.nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
        "        self.lsa = lsa\n",
        "\n",
        "    def forward(self, q, k, v, prev=None, key_padding_mask=None, attn_mask=None):\n",
        "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # Add pre-softmax attention scores from the previous layer (optional)\n",
        "        if prev is not None: attn_scores = attn_scores + prev\n",
        "\n",
        "        # Attention mask (optional)\n",
        "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
        "            else:\n",
        "                attn_scores += attn_mask\n",
        "\n",
        "        # Key padding mask (optional)\n",
        "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
        "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
        "\n",
        "        # normalize the attention weights\n",
        "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # compute the new values given the attention weights\n",
        "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2ad71ac4-db40-44b1-a7d5-43811da884fd",
      "metadata": {
        "id": "2ad71ac4-db40-44b1-a7d5-43811da884fd"
      },
      "outputs": [],
      "source": [
        "# Flatten Head Layer 구현\n",
        "\n",
        "class Flatten_Head(torch.nn.Module):\n",
        "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
        "        super().__init__()\n",
        "        self.n_vars = n_vars\n",
        "        self.flatten = torch.nn.Flatten(start_dim=-3)\n",
        "        self.linear = torch.nn.Linear(nf * n_vars, target_window)\n",
        "        self.dropout = torch.nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
        "        x = self.flatten(x)                               # x: [bs x nvars * d_model * patch_num]\n",
        "        x = self.linear(x)                                # x: [bs x target_window]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9e3b9245-41b3-4e46-b0bd-092f01fc4556",
      "metadata": {
        "id": "9e3b9245-41b3-4e46-b0bd-092f01fc4556"
      },
      "outputs": [],
      "source": [
        "# PatchTST 모델 구현\n",
        "\n",
        "class PatchTST(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window, patch_len, stride, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=16, n_heads=4, d_k=None, d_v=None,\n",
        "                 d_ff=128, attn_dropout=0.0, dropout=0.3, key_padding_mask='auto',\n",
        "                 padding_var=None, attn_mask=None, res_attention=True, pre_norm=False, store_attn=False,\n",
        "                 head_dropout = 0.0, padding_patch = \"end\",\n",
        "                 revin = True, affine = False, subtract_last = False,\n",
        "                 verbose=False, target_idx=-1, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.revin = revin\n",
        "        if revin:\n",
        "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last, target_idx=target_idx)\n",
        "\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        patch_num = int((context_window - patch_len)/stride + 1)\n",
        "\n",
        "        if padding_patch == \"end\":\n",
        "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, stride))\n",
        "            patch_num += 1\n",
        "\n",
        "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
        "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                attn_dropout=attn_dropout, dropout=dropout, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                verbose=verbose, **kwargs)\n",
        "\n",
        "        self.head_nf = d_model * patch_num\n",
        "        self.n_vars = c_in\n",
        "\n",
        "        self.head = Flatten_Head(self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
        "\n",
        "    def forward(self, z):                                                                   # z: [bs x seq_len × nvars]\n",
        "        # instance norm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, 'norm')\n",
        "            z = z.permute(0,2,1)                                                            # z: [bs x nvars × seq_len]\n",
        "\n",
        "        # do patching\n",
        "        if self.padding_patch == 'end':\n",
        "            z = self.padding_patch_layer(z)\n",
        "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
        "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        # model\n",
        "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
        "        z = self.head(z)                                                                    # z: [bs x target_window]\n",
        "\n",
        "        # denorm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, 'denorm')\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distillation_loss(student_output, teacher_output, true_output, alpha=0.5, temperature=3.0):\n",
        "    # Flatten student and teacher outputs for KL divergence\n",
        "    student_output = student_output.view(student_output.size(0), -1)  # [batch_size, num_features * pred_len]\n",
        "    teacher_output = teacher_output.view(teacher_output.size(0), -1)  # [batch_size, num_features * pred_len]\n",
        "\n",
        "    # Compute distillation loss\n",
        "    distill_loss = F.kl_div(\n",
        "        F.log_softmax(student_output / temperature, dim=1),\n",
        "        F.softmax(teacher_output / temperature, dim=1),\n",
        "        reduction=\"batchmean\",\n",
        "    ) * (temperature ** 2)\n",
        "\n",
        "    # Compute MSE loss\n",
        "    true_output = true_output.view(true_output.size(0), -1)  # [batch_size, num_features * pred_len]\n",
        "    mse_loss = F.mse_loss(student_output, true_output)\n",
        "\n",
        "    # Combine losses\n",
        "    return alpha * distill_loss + (1 - alpha) * mse_loss\n"
      ],
      "metadata": {
        "id": "UY5Vhsdxr3EP"
      },
      "id": "UY5Vhsdxr3EP",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "17d94656-c97e-47cc-9b03-86aa37318c10",
      "metadata": {
        "id": "17d94656-c97e-47cc-9b03-86aa37318c10"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 비교를 위한 Linear model 정의\n",
        "\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window):\n",
        "        super().__init__()\n",
        "        self.c_in = c_in\n",
        "        self.context_winsoq = context_window\n",
        "        self.target_window = target_window\n",
        "\n",
        "        self.flatten = torch.nn.Flatten(start_dim=-2)\n",
        "\n",
        "        self.linear = torch.nn.Linear(c_in * context_window, target_window)\n",
        "\n",
        "    def forward(self, x):                   # x: [bs x seq_len × nvars]\n",
        "        x = self.flatten(x)                 # x: [bs x seq_len * nvars]\n",
        "        x = self.linear(x)                  # x: [bs x target_window]\n",
        "        return x\n",
        "\n",
        "\n",
        "class moving_avg(torch.nn.Module):\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(torch.nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "class DLinear(torch.nn.Module):\n",
        "    def __init__(self, c_in, context_window, target_window):\n",
        "        super().__init__()\n",
        "        # Decompsition Kernel Size\n",
        "        kernel_size = 25\n",
        "        self.decompsition = series_decomp(kernel_size)\n",
        "        self.flatten_Seasonal = torch.nn.Flatten(start_dim=-2)\n",
        "        self.flatten_Trend = torch.nn.Flatten(start_dim=-2)\n",
        "\n",
        "        self.Linear_Seasonal = torch.nn.Linear(c_in * context_window, target_window)\n",
        "        self.Linear_Trend = torch.nn.Linear(c_in * context_window, target_window)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Input length, Channel]\n",
        "        seasonal_init, trend_init = self.decompsition(x)\n",
        "        seasonal_init = self.flatten_Seasonal(x)\n",
        "        trend_init = self.flatten_Trend(x)\n",
        "\n",
        "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
        "        trend_output = self.Linear_Trend(trend_init)\n",
        "\n",
        "        x = seasonal_output + trend_output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "156bddb1-e7ea-4812-81d3-881a667b4f3a",
      "metadata": {
        "id": "156bddb1-e7ea-4812-81d3-881a667b4f3a"
      },
      "outputs": [],
      "source": [
        "# 모델의 학습, 검증, 테스트\n",
        "\n",
        "class Learner:\n",
        "    def __init__(self, model, dataset, batch_size=128, lr=0.0001, epochs=100, target_window=96, d_model=16, adjust_lr=True, adjust_factor=0.001):\n",
        "        self.model = model.to(\"cuda\")\n",
        "        self.batch_size = batch_size\n",
        "        train_dataset = dataset(mode=\"train\")\n",
        "        valid_dataset = dataset(mode=\"val\")\n",
        "        test_dataset = dataset(mode=\"test\")\n",
        "        self.train_datalen = len(train_dataset)\n",
        "        self.valid_datalen = len(valid_dataset)\n",
        "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.lr=lr\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.loss = torch.nn.MSELoss()\n",
        "        self.epochs = epochs\n",
        "        self.target_window=target_window\n",
        "        self.best_weight = self.model.state_dict()\n",
        "        self.d_model=d_model\n",
        "        self.adjust_lr = adjust_lr\n",
        "        self.adjust_factor = adjust_factor\n",
        "\n",
        "    def adjust_learning_rate(self, steps, warmup_step=300, printout=True):\n",
        "        if steps**(-0.5) < steps * (warmup_step**-1.5):\n",
        "            lr_adjust = (16**-0.5) * (steps**-0.5) * self.adjust_factor\n",
        "        else:\n",
        "            lr_adjust = (16**-0.5) * (steps * (warmup_step**-1.5)) * self.adjust_factor\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr_adjust\n",
        "        if printout:\n",
        "            print('Updating learning rate to {}'.format(lr_adjust))\n",
        "        return\n",
        "\n",
        "    def train(self):\n",
        "        best_valid_loss = np.inf\n",
        "        train_history = []\n",
        "        valid_history = []\n",
        "        train_steps = 1\n",
        "        if self.adjust_lr:\n",
        "            self.adjust_learning_rate(train_steps)\n",
        "        for epoch in range(self.epochs):\n",
        "            #train\n",
        "            self.model.train()\n",
        "            iter_count = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for train_x, train_y in self.train_dataloader:\n",
        "                train_x = train_x.to(\"cuda\")\n",
        "                train_y = train_y.to(\"cuda\")\n",
        "\n",
        "                pred_y = self.model(train_x)\n",
        "                loss = self.loss(pred_y, train_y)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                iter_count += 1\n",
        "                train_steps += 1\n",
        "            if self.adjust_lr:\n",
        "                self.adjust_learning_rate(train_steps)\n",
        "\n",
        "            #valid\n",
        "            self.model.eval()\n",
        "            valid_iter_count = 0\n",
        "            valid_total_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for valid_x, valid_y in self.valid_dataloader:\n",
        "                    valid_x = valid_x.to(\"cuda\")\n",
        "                    valid_y = valid_y.to(\"cuda\")\n",
        "                    pred_y = self.model(valid_x)\n",
        "                    loss = self.loss(pred_y, valid_y)\n",
        "                    valid_total_loss += loss.item()\n",
        "                    valid_iter_count += 1\n",
        "\n",
        "            total_loss /= iter_count\n",
        "            valid_total_loss /= valid_iter_count\n",
        "            print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
        "            if best_valid_loss >= valid_total_loss:\n",
        "                self.best_weight = self.model.state_dict()\n",
        "                best_valid_loss = valid_total_loss\n",
        "                print(\"Best score! Weights of the model are updated!\")\n",
        "            train_history.append(total_loss)\n",
        "            valid_history.append(valid_total_loss)\n",
        "        return train_history, valid_history\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(self.best_weight)\n",
        "        self.model.eval()\n",
        "        iter_count = 0\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for test_x, test_y in self.test_dataloader:\n",
        "                test_x = test_x.to(\"cuda\")\n",
        "                test_y = test_y.to(\"cuda\")\n",
        "                pred_y = self.model(test_x)\n",
        "                loss = self.loss(pred_y, test_y)\n",
        "                total_loss += loss.item()\n",
        "                iter_count += 1\n",
        "        total_loss /= iter_count\n",
        "        print(\"MSE test loss: {:.4f}\".format(total_loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationLearner(Learner):\n",
        "    def __init__(self, teacher_model, student_model, dataset, alpha=0.5, temperature=3.0, **kwargs):\n",
        "        # 교사 모델 초기화 및 학습 불가능하게 설정\n",
        "        self.teacher_model = deepcopy(teacher_model).to(\"cuda\")\n",
        "        for param in self.teacher_model.parameters():\n",
        "            param.requires_grad = False  # 교사 모델은 학습되지 않도록 설정\n",
        "\n",
        "        # 부모 클래스 초기화\n",
        "        super().__init__(model=student_model, dataset=dataset, **kwargs)\n",
        "\n",
        "        # 증류 학습에 필요한 파라미터 설정\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train(self):\n",
        "        best_valid_loss = float(\"inf\")\n",
        "        train_history = []\n",
        "        valid_history = []\n",
        "        train_steps = 1\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            self.teacher_model.eval()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch_idx, (train_x, train_y) in enumerate(self.train_dataloader):\n",
        "                train_x, train_y = train_x.to(\"cuda\"), train_y.to(\"cuda\")\n",
        "\n",
        "                # 학생 모델 예측\n",
        "                student_pred = self.model(train_x)\n",
        "\n",
        "                # 교사 모델 예측\n",
        "                with torch.no_grad():\n",
        "                    teacher_pred = self.teacher_model(train_x)\n",
        "\n",
        "                # 증류 손실 계산\n",
        "                loss = distillation_loss(student_pred, teacher_pred, train_y, self.alpha, self.temperature)\n",
        "\n",
        "                # 역전파 및 가중치 업데이트\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                # 학습률 조정\n",
        "                if self.adjust_lr:\n",
        "                    self.adjust_learning_rate(train_steps, printout=False)\n",
        "                train_steps += 1\n",
        "\n",
        "            # 학습 손실 기록\n",
        "            avg_train_loss = epoch_loss / len(self.train_dataloader)\n",
        "\n",
        "            # 검증 손실 계산\n",
        "            self.model.eval()\n",
        "            valid_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for valid_x, valid_y in self.valid_dataloader:\n",
        "                    valid_x, valid_y = valid_x.to(\"cuda\"), valid_y.to(\"cuda\")\n",
        "                    pred_y = self.model(valid_x)\n",
        "                    valid_loss += self.loss(pred_y, valid_y).item()\n",
        "            avg_valid_loss = valid_loss / len(self.valid_dataloader)\n",
        "\n",
        "            print(f\"epoch: {epoch + 1} MSE loss: {avg_train_loss:.4f} MSE valid loss: {avg_valid_loss:.4f}\")\n",
        "\n",
        "            if best_valid_loss >= avg_valid_loss:\n",
        "                self.best_weight = self.model.state_dict()\n",
        "                best_valid_loss = avg_valid_loss\n",
        "\n",
        "            train_history.append(avg_train_loss)\n",
        "            valid_history.append(avg_valid_loss)\n",
        "\n",
        "        return train_history, valid_history\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(self.best_weight)\n",
        "        self.model.eval()\n",
        "        iter_count = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_x, test_y in self.test_dataloader:\n",
        "                test_x = test_x.to(\"cuda\")\n",
        "                test_y = test_y.to(\"cuda\")\n",
        "                pred_y = self.model(test_x)\n",
        "                loss = self.loss(pred_y, test_y)\n",
        "                total_loss += loss.item()\n",
        "                iter_count += 1\n",
        "\n",
        "        total_loss /= iter_count\n",
        "        print(f\"MSE test loss: {total_loss:.4f}\")\n",
        "        return total_loss\n",
        "\n",
        "    def adjust_learning_rate(self, steps, warmup_step=300, printout=False):\n",
        "        if steps**(-0.5) < steps * (warmup_step**-1.5):\n",
        "            lr_adjust = (self.d_model**-0.5) * (steps**-0.5) * self.adjust_factor\n",
        "        else:\n",
        "            lr_adjust = (self.d_model**-0.5) * (steps * (warmup_step**-1.5)) * self.adjust_factor\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr_adjust\n",
        "        if printout:\n",
        "            print(f'Updating learning rate to {lr_adjust}')\n",
        "        return\n"
      ],
      "metadata": {
        "id": "sqwDtcm5taAS"
      },
      "id": "sqwDtcm5taAS",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "c_in = 7\n",
        "context_window = 336\n",
        "target_window = 96\n",
        "patch_len = 16\n",
        "stride = 8"
      ],
      "metadata": {
        "id": "Yh0D2YSVx6Qa"
      },
      "id": "Yh0D2YSVx6Qa",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, num_features, seq_len, pred_len, hidden_dim, patch_len, stride, n_layers=2, d_model=16, n_heads=2, dropout=0.1):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "\n",
        "        # Patch 계산\n",
        "        self.patch_num = int((seq_len - patch_len) / stride + 1)\n",
        "\n",
        "        # 입력 프로젝션: 패치 데이터를 d_model 차원으로 매핑\n",
        "        self.input_proj = nn.Linear(patch_len * num_features, d_model)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=d_model,\n",
        "                nhead=n_heads,\n",
        "                dim_feedforward=d_model * 4,\n",
        "                dropout=dropout,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        # 예측 Head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model * self.patch_num, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, pred_len)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, num_features]\n",
        "        batch_size, seq_len, num_features = x.shape\n",
        "\n",
        "        # 패치 생성\n",
        "        x = x.unfold(dimension=1, size=self.patch_len, step=self.stride)  # [batch_size, patch_num, patch_len, num_features]\n",
        "        x = x.permute(0, 1, 3, 2)  # [batch_size, patch_num, num_features, patch_len]\n",
        "        x = x.flatten(start_dim=2)  # [batch_size, patch_num, patch_len * num_features]\n",
        "\n",
        "        # 입력 프로젝션\n",
        "        x = self.input_proj(x)  # [batch_size, patch_num, d_model]\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = self.encoder(x)  # [batch_size, patch_num, d_model]\n",
        "\n",
        "        # Flatten 및 예측\n",
        "        x = x.flatten(start_dim=1)  # [batch_size, patch_num * d_model]\n",
        "        x = self.head(x)  # [batch_size, pred_len]\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zBgdkiVgGoeW"
      },
      "id": "zBgdkiVgGoeW",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "cFejdRu9xjp_"
      },
      "id": "cFejdRu9xjp_",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "513d5644-ed43-4baa-b551-b022909ba2d2",
      "metadata": {
        "id": "513d5644-ed43-4baa-b551-b022909ba2d2"
      },
      "outputs": [],
      "source": [
        "patchtst_model = PatchTST(c_in=c_in, context_window=context_window, target_window=target_window, patch_len=patch_len, stride=stride)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = c_in           # 입력 feature 수\n",
        "seq_len = context_window       # 입력 시계열 길이\n",
        "pred_len = target_window       # 예측 시계열 길이\n",
        "hidden_dim = 128               # 학생 모델 은닉층 크기\n",
        "patch_len = 16                 # 패치 길이\n",
        "stride = 8                     # 스트라이드\n",
        "\n",
        "teacher_model = patchtst_model\n",
        "\n",
        "student_model = StudentModel(\n",
        "    num_features=num_features,  # 입력 feature 수\n",
        "    seq_len=seq_len,            # 입력 시계열 길이\n",
        "    pred_len=pred_len,          # 예측 시계열 길이\n",
        "    hidden_dim=hidden_dim,      # 은닉층 크기\n",
        "    patch_len=patch_len,        # 패치 길이\n",
        "    stride=stride,              # 스트라이드\n",
        "    n_layers=2,                 # Transformer 레이어 수\n",
        "    d_model=16,                 # 모델 차원\n",
        "    n_heads=2,                  # 헤드 수\n",
        "    dropout=0.1                 # 드롭아웃 비율\n",
        ")"
      ],
      "metadata": {
        "id": "I8Z1b0B6Hpqo"
      },
      "id": "I8Z1b0B6Hpqo",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "35074b57-1b94-4e8b-ba2d-bf71a10d851a",
      "metadata": {
        "id": "35074b57-1b94-4e8b-ba2d-bf71a10d851a"
      },
      "outputs": [],
      "source": [
        "Linear_model = Linear(c_in=c_in, context_window=context_window, target_window=target_window)\n",
        "DLinear_model = DLinear(c_in=c_in, context_window=context_window, target_window=target_window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "12030895-2232-41e5-843d-fe5e7caf5726",
      "metadata": {
        "scrolled": true,
        "id": "12030895-2232-41e5-843d-fe5e7caf5726"
      },
      "outputs": [],
      "source": [
        "Linear_learner = Learner(model=Linear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
        "DLinear_learner = Learner(model=DLinear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
        "patchtst_learner = Learner(model=patchtst_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.001)\n",
        "student_learner = Learner(model=student_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "103ed611-bc5f-4926-aa39-35d94e01ab38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "103ed611-bc5f-4926-aa39-35d94e01ab38",
        "outputId": "dc349a3b-9550-47b7-aa47-a9757c44bd08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating learning rate to 4.811252243246882e-07\n",
            "Updating learning rate to 3.1754264805429416e-05\n",
            "epoch: 0 MSE loss: 1.1685 MSE valid loss: 0.8118\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.302740438653415e-05\n",
            "epoch: 1 MSE loss: 0.5663 MSE valid loss: 0.6749\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.430054396763888e-05\n",
            "epoch: 2 MSE loss: 0.3087 MSE valid loss: 0.5496\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00012557368354874362\n",
            "epoch: 3 MSE loss: 0.2410 MSE valid loss: 0.4157\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00013846219390542782\n",
            "epoch: 4 MSE loss: 0.2072 MSE valid loss: 0.2954\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.0001264304343560434\n",
            "epoch: 5 MSE loss: 0.1872 MSE valid loss: 0.2646\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00011707322644771175\n",
            "epoch: 6 MSE loss: 0.1755 MSE valid loss: 0.2602\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00010952698858458088\n",
            "epoch: 7 MSE loss: 0.1685 MSE valid loss: 0.2539\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00010327404809650345\n",
            "epoch: 8 MSE loss: 0.1628 MSE valid loss: 0.2534\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.798272520870258e-05\n",
            "epoch: 9 MSE loss: 0.1582 MSE valid loss: 0.2562\n",
            "Updating learning rate to 9.342938659399198e-05\n",
            "epoch: 10 MSE loss: 0.1554 MSE valid loss: 0.2558\n",
            "Updating learning rate to 8.945703337056415e-05\n",
            "epoch: 11 MSE loss: 0.1523 MSE valid loss: 0.2553\n",
            "Updating learning rate to 8.595177052156611e-05\n",
            "epoch: 12 MSE loss: 0.1502 MSE valid loss: 0.2532\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 8.282869524032146e-05\n",
            "epoch: 13 MSE loss: 0.1481 MSE valid loss: 0.2578\n",
            "Updating learning rate to 8.002304995805999e-05\n",
            "epoch: 14 MSE loss: 0.1462 MSE valid loss: 0.2564\n",
            "Updating learning rate to 7.748446592171796e-05\n",
            "epoch: 15 MSE loss: 0.1457 MSE valid loss: 0.2519\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 7.517309741553296e-05\n",
            "epoch: 16 MSE loss: 0.1431 MSE valid loss: 0.2500\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 7.305695402335044e-05\n",
            "epoch: 17 MSE loss: 0.1423 MSE valid loss: 0.2505\n",
            "Updating learning rate to 7.111001549857179e-05\n",
            "epoch: 18 MSE loss: 0.1409 MSE valid loss: 0.2488\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.931087162517846e-05\n",
            "epoch: 19 MSE loss: 0.1400 MSE valid loss: 0.2475\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.76417225936176e-05\n",
            "epoch: 20 MSE loss: 0.1392 MSE valid loss: 0.2502\n",
            "Updating learning rate to 6.608763214317867e-05\n",
            "epoch: 21 MSE loss: 0.1380 MSE valid loss: 0.2469\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.46359612493774e-05\n",
            "epoch: 22 MSE loss: 0.1375 MSE valid loss: 0.2471\n",
            "Updating learning rate to 6.327593294406921e-05\n",
            "epoch: 23 MSE loss: 0.1368 MSE valid loss: 0.2463\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.199829383043036e-05\n",
            "epoch: 24 MSE loss: 0.1352 MSE valid loss: 0.2430\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.079504788572469e-05\n",
            "epoch: 25 MSE loss: 0.1351 MSE valid loss: 0.2508\n",
            "Updating learning rate to 5.965924498791846e-05\n",
            "epoch: 26 MSE loss: 0.1342 MSE valid loss: 0.2480\n",
            "Updating learning rate to 5.8584811349126495e-05\n",
            "epoch: 27 MSE loss: 0.1341 MSE valid loss: 0.2493\n",
            "Updating learning rate to 5.7566412382313e-05\n",
            "epoch: 28 MSE loss: 0.1332 MSE valid loss: 0.2463\n",
            "Updating learning rate to 5.659934091583233e-05\n",
            "epoch: 29 MSE loss: 0.1321 MSE valid loss: 0.2456\n",
            "Updating learning rate to 5.567942539842175e-05\n",
            "epoch: 30 MSE loss: 0.1316 MSE valid loss: 0.2454\n",
            "Updating learning rate to 5.4802954002676805e-05\n",
            "epoch: 31 MSE loss: 0.1313 MSE valid loss: 0.2442\n",
            "Updating learning rate to 5.39666114720432e-05\n",
            "epoch: 32 MSE loss: 0.1300 MSE valid loss: 0.2469\n",
            "Updating learning rate to 5.316742625738918e-05\n",
            "epoch: 33 MSE loss: 0.1299 MSE valid loss: 0.2456\n",
            "Updating learning rate to 5.240272601878982e-05\n",
            "epoch: 34 MSE loss: 0.1296 MSE valid loss: 0.2463\n",
            "Updating learning rate to 5.1670099971819504e-05\n",
            "epoch: 35 MSE loss: 0.1294 MSE valid loss: 0.2415\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 5.096736686795811e-05\n",
            "epoch: 36 MSE loss: 0.1289 MSE valid loss: 0.2473\n",
            "Updating learning rate to 5.0292547639160534e-05\n",
            "epoch: 37 MSE loss: 0.1281 MSE valid loss: 0.2424\n",
            "Updating learning rate to 4.96438419243461e-05\n",
            "epoch: 38 MSE loss: 0.1282 MSE valid loss: 0.2404\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 4.901960784313725e-05\n",
            "epoch: 39 MSE loss: 0.1274 MSE valid loss: 0.2417\n",
            "Updating learning rate to 4.841834449897069e-05\n",
            "epoch: 40 MSE loss: 0.1273 MSE valid loss: 0.2418\n",
            "Updating learning rate to 4.783867678672252e-05\n",
            "epoch: 41 MSE loss: 0.1270 MSE valid loss: 0.2428\n",
            "Updating learning rate to 4.727934215451461e-05\n",
            "epoch: 42 MSE loss: 0.1263 MSE valid loss: 0.2421\n",
            "Updating learning rate to 4.6739179029417445e-05\n",
            "epoch: 43 MSE loss: 0.1259 MSE valid loss: 0.2421\n",
            "Updating learning rate to 4.621711666540848e-05\n",
            "epoch: 44 MSE loss: 0.1255 MSE valid loss: 0.2455\n",
            "Updating learning rate to 4.571216621155238e-05\n",
            "epoch: 45 MSE loss: 0.1253 MSE valid loss: 0.2443\n",
            "Updating learning rate to 4.522341283077635e-05\n",
            "epoch: 46 MSE loss: 0.1246 MSE valid loss: 0.2448\n",
            "Updating learning rate to 4.4750008726252546e-05\n",
            "epoch: 47 MSE loss: 0.1252 MSE valid loss: 0.2433\n",
            "Updating learning rate to 4.4291166954394496e-05\n",
            "epoch: 48 MSE loss: 0.1253 MSE valid loss: 0.2409\n",
            "Updating learning rate to 4.384615592171157e-05\n",
            "epoch: 49 MSE loss: 0.1242 MSE valid loss: 0.2409\n",
            "Updating learning rate to 4.341429447794924e-05\n",
            "epoch: 50 MSE loss: 0.1242 MSE valid loss: 0.2426\n",
            "Updating learning rate to 4.299494753063186e-05\n",
            "epoch: 51 MSE loss: 0.1238 MSE valid loss: 0.2381\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 4.2587522116770666e-05\n",
            "epoch: 52 MSE loss: 0.1234 MSE valid loss: 0.2418\n",
            "Updating learning rate to 4.219146387646126e-05\n",
            "epoch: 53 MSE loss: 0.1230 MSE valid loss: 0.2430\n",
            "Updating learning rate to 4.1806253880665696e-05\n",
            "epoch: 54 MSE loss: 0.1233 MSE valid loss: 0.2428\n",
            "Updating learning rate to 4.143140577189121e-05\n",
            "epoch: 55 MSE loss: 0.1225 MSE valid loss: 0.2370\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 4.106646318193315e-05\n",
            "epoch: 56 MSE loss: 0.1221 MSE valid loss: 0.2451\n",
            "Updating learning rate to 4.071099739550263e-05\n",
            "epoch: 57 MSE loss: 0.1221 MSE valid loss: 0.2457\n",
            "Updating learning rate to 4.0364605232539164e-05\n",
            "epoch: 58 MSE loss: 0.1220 MSE valid loss: 0.2415\n",
            "Updating learning rate to 4.0026907125422175e-05\n",
            "epoch: 59 MSE loss: 0.1217 MSE valid loss: 0.2417\n",
            "Updating learning rate to 3.969754537023144e-05\n",
            "epoch: 60 MSE loss: 0.1214 MSE valid loss: 0.2398\n",
            "Updating learning rate to 3.937618253373847e-05\n",
            "epoch: 61 MSE loss: 0.1212 MSE valid loss: 0.2459\n",
            "Updating learning rate to 3.90625e-05\n",
            "epoch: 62 MSE loss: 0.1211 MSE valid loss: 0.2438\n",
            "Updating learning rate to 3.875619664232189e-05\n",
            "epoch: 63 MSE loss: 0.1205 MSE valid loss: 0.2430\n",
            "Updating learning rate to 3.845698760800996e-05\n",
            "epoch: 64 MSE loss: 0.1206 MSE valid loss: 0.2435\n",
            "Updating learning rate to 3.816460320475954e-05\n",
            "epoch: 65 MSE loss: 0.1197 MSE valid loss: 0.2431\n",
            "Updating learning rate to 3.787878787878788e-05\n",
            "epoch: 66 MSE loss: 0.1200 MSE valid loss: 0.2453\n",
            "Updating learning rate to 3.759929927590882e-05\n",
            "epoch: 67 MSE loss: 0.1197 MSE valid loss: 0.2429\n",
            "Updating learning rate to 3.732590737770921e-05\n",
            "epoch: 68 MSE loss: 0.1195 MSE valid loss: 0.2424\n",
            "Updating learning rate to 3.7058393705829335e-05\n",
            "epoch: 69 MSE loss: 0.1197 MSE valid loss: 0.2422\n",
            "Updating learning rate to 3.679655058809149e-05\n",
            "epoch: 70 MSE loss: 0.1193 MSE valid loss: 0.2437\n",
            "Updating learning rate to 3.654018048087443e-05\n",
            "epoch: 71 MSE loss: 0.1191 MSE valid loss: 0.2456\n",
            "Updating learning rate to 3.6289095342709304e-05\n",
            "epoch: 72 MSE loss: 0.1193 MSE valid loss: 0.2430\n",
            "Updating learning rate to 3.604311605458291e-05\n",
            "epoch: 73 MSE loss: 0.1186 MSE valid loss: 0.2464\n",
            "Updating learning rate to 3.58020718828878e-05\n",
            "epoch: 74 MSE loss: 0.1185 MSE valid loss: 0.2456\n",
            "Updating learning rate to 3.5565799981359994e-05\n",
            "epoch: 75 MSE loss: 0.1179 MSE valid loss: 0.2436\n",
            "Updating learning rate to 3.5334144928703015e-05\n",
            "epoch: 76 MSE loss: 0.1180 MSE valid loss: 0.2457\n",
            "Updating learning rate to 3.510695829891526e-05\n",
            "epoch: 77 MSE loss: 0.1186 MSE valid loss: 0.2400\n",
            "Updating learning rate to 3.488409826162172e-05\n",
            "epoch: 78 MSE loss: 0.1186 MSE valid loss: 0.2447\n",
            "Updating learning rate to 3.4665429209964995e-05\n",
            "epoch: 79 MSE loss: 0.1182 MSE valid loss: 0.2453\n",
            "Updating learning rate to 3.445082141383733e-05\n",
            "epoch: 80 MSE loss: 0.1181 MSE valid loss: 0.2444\n",
            "Updating learning rate to 3.424015069643934e-05\n",
            "epoch: 81 MSE loss: 0.1175 MSE valid loss: 0.2433\n",
            "Updating learning rate to 3.4033298132333136e-05\n",
            "epoch: 82 MSE loss: 0.1172 MSE valid loss: 0.2450\n",
            "Updating learning rate to 3.383014976532195e-05\n",
            "epoch: 83 MSE loss: 0.1173 MSE valid loss: 0.2429\n",
            "Updating learning rate to 3.3630596344635906e-05\n",
            "epoch: 84 MSE loss: 0.1168 MSE valid loss: 0.2466\n",
            "Updating learning rate to 3.343453307803635e-05\n",
            "epoch: 85 MSE loss: 0.1174 MSE valid loss: 0.2439\n",
            "Updating learning rate to 3.3241859400571366e-05\n",
            "epoch: 86 MSE loss: 0.1166 MSE valid loss: 0.2476\n",
            "Updating learning rate to 3.305247875782325e-05\n",
            "epoch: 87 MSE loss: 0.1170 MSE valid loss: 0.2458\n",
            "Updating learning rate to 3.286629840258662e-05\n",
            "epoch: 88 MSE loss: 0.1168 MSE valid loss: 0.2464\n",
            "Updating learning rate to 3.268322920400467e-05\n",
            "epoch: 89 MSE loss: 0.1168 MSE valid loss: 0.2470\n",
            "Updating learning rate to 3.250318546827149e-05\n",
            "epoch: 90 MSE loss: 0.1170 MSE valid loss: 0.2465\n",
            "Updating learning rate to 3.232608477008077e-05\n",
            "epoch: 91 MSE loss: 0.1167 MSE valid loss: 0.2477\n",
            "Updating learning rate to 3.21518477940684e-05\n",
            "epoch: 92 MSE loss: 0.1167 MSE valid loss: 0.2448\n",
            "Updating learning rate to 3.198039818555568e-05\n",
            "epoch: 93 MSE loss: 0.1159 MSE valid loss: 0.2450\n",
            "Updating learning rate to 3.181166240995547e-05\n",
            "epoch: 94 MSE loss: 0.1163 MSE valid loss: 0.2438\n",
            "Updating learning rate to 3.1645569620253167e-05\n",
            "epoch: 95 MSE loss: 0.1159 MSE valid loss: 0.2435\n",
            "Updating learning rate to 3.148205153202001e-05\n",
            "epoch: 96 MSE loss: 0.1160 MSE valid loss: 0.2473\n",
            "Updating learning rate to 3.1321042305458135e-05\n",
            "epoch: 97 MSE loss: 0.1150 MSE valid loss: 0.2458\n",
            "Updating learning rate to 3.1162478434014266e-05\n",
            "epoch: 98 MSE loss: 0.1151 MSE valid loss: 0.2479\n",
            "Updating learning rate to 3.100629863913435e-05\n",
            "epoch: 99 MSE loss: 0.1157 MSE valid loss: 0.2476\n",
            "Updating learning rate to 4.811252243246882e-07\n",
            "Updating learning rate to 3.1754264805429416e-05\n",
            "epoch: 0 MSE loss: 1.3364 MSE valid loss: 1.2570\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.302740438653415e-05\n",
            "epoch: 1 MSE loss: 0.5755 MSE valid loss: 0.8456\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.430054396763888e-05\n",
            "epoch: 2 MSE loss: 0.3285 MSE valid loss: 0.5207\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00012557368354874362\n",
            "epoch: 3 MSE loss: 0.2546 MSE valid loss: 0.3553\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00013846219390542782\n",
            "epoch: 4 MSE loss: 0.2167 MSE valid loss: 0.3052\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.0001264304343560434\n",
            "epoch: 5 MSE loss: 0.1935 MSE valid loss: 0.3027\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00011707322644771175\n",
            "epoch: 6 MSE loss: 0.1813 MSE valid loss: 0.2957\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00010952698858458088\n",
            "epoch: 7 MSE loss: 0.1717 MSE valid loss: 0.2914\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 0.00010327404809650345\n",
            "epoch: 8 MSE loss: 0.1660 MSE valid loss: 0.2803\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.798272520870258e-05\n",
            "epoch: 9 MSE loss: 0.1618 MSE valid loss: 0.2789\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.342938659399198e-05\n",
            "epoch: 10 MSE loss: 0.1582 MSE valid loss: 0.2795\n",
            "Updating learning rate to 8.945703337056415e-05\n",
            "epoch: 11 MSE loss: 0.1544 MSE valid loss: 0.2691\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 8.595177052156611e-05\n",
            "epoch: 12 MSE loss: 0.1520 MSE valid loss: 0.2648\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 8.282869524032146e-05\n",
            "epoch: 13 MSE loss: 0.1495 MSE valid loss: 0.2704\n",
            "Updating learning rate to 8.002304995805999e-05\n",
            "epoch: 14 MSE loss: 0.1471 MSE valid loss: 0.2634\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 7.748446592171796e-05\n",
            "epoch: 15 MSE loss: 0.1461 MSE valid loss: 0.2677\n",
            "Updating learning rate to 7.517309741553296e-05\n",
            "epoch: 16 MSE loss: 0.1437 MSE valid loss: 0.2642\n",
            "Updating learning rate to 7.305695402335044e-05\n",
            "epoch: 17 MSE loss: 0.1423 MSE valid loss: 0.2685\n",
            "Updating learning rate to 7.111001549857179e-05\n",
            "epoch: 18 MSE loss: 0.1410 MSE valid loss: 0.2657\n",
            "Updating learning rate to 6.931087162517846e-05\n",
            "epoch: 19 MSE loss: 0.1407 MSE valid loss: 0.2661\n",
            "Updating learning rate to 6.76417225936176e-05\n",
            "epoch: 20 MSE loss: 0.1393 MSE valid loss: 0.2604\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.608763214317867e-05\n",
            "epoch: 21 MSE loss: 0.1375 MSE valid loss: 0.2588\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.46359612493774e-05\n",
            "epoch: 22 MSE loss: 0.1368 MSE valid loss: 0.2596\n",
            "Updating learning rate to 6.327593294406921e-05\n",
            "epoch: 23 MSE loss: 0.1365 MSE valid loss: 0.2599\n",
            "Updating learning rate to 6.199829383043036e-05\n",
            "epoch: 24 MSE loss: 0.1349 MSE valid loss: 0.2610\n",
            "Updating learning rate to 6.079504788572469e-05\n",
            "epoch: 25 MSE loss: 0.1341 MSE valid loss: 0.2645\n",
            "Updating learning rate to 5.965924498791846e-05\n",
            "epoch: 26 MSE loss: 0.1343 MSE valid loss: 0.2614\n",
            "Updating learning rate to 5.8584811349126495e-05\n",
            "epoch: 27 MSE loss: 0.1332 MSE valid loss: 0.2509\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 5.7566412382313e-05\n",
            "epoch: 28 MSE loss: 0.1316 MSE valid loss: 0.2553\n",
            "Updating learning rate to 5.659934091583233e-05\n",
            "epoch: 29 MSE loss: 0.1312 MSE valid loss: 0.2548\n",
            "Updating learning rate to 5.567942539842175e-05\n",
            "epoch: 30 MSE loss: 0.1304 MSE valid loss: 0.2627\n",
            "Updating learning rate to 5.4802954002676805e-05\n",
            "epoch: 31 MSE loss: 0.1309 MSE valid loss: 0.2629\n",
            "Updating learning rate to 5.39666114720432e-05\n",
            "epoch: 32 MSE loss: 0.1290 MSE valid loss: 0.2610\n",
            "Updating learning rate to 5.316742625738918e-05\n",
            "epoch: 33 MSE loss: 0.1287 MSE valid loss: 0.2590\n",
            "Updating learning rate to 5.240272601878982e-05\n",
            "epoch: 34 MSE loss: 0.1279 MSE valid loss: 0.2608\n",
            "Updating learning rate to 5.1670099971819504e-05\n",
            "epoch: 35 MSE loss: 0.1281 MSE valid loss: 0.2575\n",
            "Updating learning rate to 5.096736686795811e-05\n",
            "epoch: 36 MSE loss: 0.1266 MSE valid loss: 0.2549\n",
            "Updating learning rate to 5.0292547639160534e-05\n",
            "epoch: 37 MSE loss: 0.1269 MSE valid loss: 0.2576\n",
            "Updating learning rate to 4.96438419243461e-05\n",
            "epoch: 38 MSE loss: 0.1261 MSE valid loss: 0.2621\n",
            "Updating learning rate to 4.901960784313725e-05\n",
            "epoch: 39 MSE loss: 0.1259 MSE valid loss: 0.2600\n",
            "Updating learning rate to 4.841834449897069e-05\n",
            "epoch: 40 MSE loss: 0.1253 MSE valid loss: 0.2588\n",
            "Updating learning rate to 4.783867678672252e-05\n",
            "epoch: 41 MSE loss: 0.1249 MSE valid loss: 0.2597\n",
            "Updating learning rate to 4.727934215451461e-05\n",
            "epoch: 42 MSE loss: 0.1243 MSE valid loss: 0.2553\n",
            "Updating learning rate to 4.6739179029417445e-05\n",
            "epoch: 43 MSE loss: 0.1239 MSE valid loss: 0.2635\n",
            "Updating learning rate to 4.621711666540848e-05\n",
            "epoch: 44 MSE loss: 0.1236 MSE valid loss: 0.2558\n",
            "Updating learning rate to 4.571216621155238e-05\n",
            "epoch: 45 MSE loss: 0.1239 MSE valid loss: 0.2621\n",
            "Updating learning rate to 4.522341283077635e-05\n",
            "epoch: 46 MSE loss: 0.1227 MSE valid loss: 0.2653\n",
            "Updating learning rate to 4.4750008726252546e-05\n",
            "epoch: 47 MSE loss: 0.1226 MSE valid loss: 0.2622\n",
            "Updating learning rate to 4.4291166954394496e-05\n",
            "epoch: 48 MSE loss: 0.1224 MSE valid loss: 0.2612\n",
            "Updating learning rate to 4.384615592171157e-05\n",
            "epoch: 49 MSE loss: 0.1216 MSE valid loss: 0.2568\n",
            "Updating learning rate to 4.341429447794924e-05\n",
            "epoch: 50 MSE loss: 0.1215 MSE valid loss: 0.2567\n",
            "Updating learning rate to 4.299494753063186e-05\n",
            "epoch: 51 MSE loss: 0.1215 MSE valid loss: 0.2618\n",
            "Updating learning rate to 4.2587522116770666e-05\n",
            "epoch: 52 MSE loss: 0.1208 MSE valid loss: 0.2538\n",
            "Updating learning rate to 4.219146387646126e-05\n",
            "epoch: 53 MSE loss: 0.1210 MSE valid loss: 0.2644\n",
            "Updating learning rate to 4.1806253880665696e-05\n",
            "epoch: 54 MSE loss: 0.1207 MSE valid loss: 0.2686\n",
            "Updating learning rate to 4.143140577189121e-05\n",
            "epoch: 55 MSE loss: 0.1202 MSE valid loss: 0.2644\n",
            "Updating learning rate to 4.106646318193315e-05\n",
            "epoch: 56 MSE loss: 0.1203 MSE valid loss: 0.2640\n",
            "Updating learning rate to 4.071099739550263e-05\n",
            "epoch: 57 MSE loss: 0.1196 MSE valid loss: 0.2610\n",
            "Updating learning rate to 4.0364605232539164e-05\n",
            "epoch: 58 MSE loss: 0.1195 MSE valid loss: 0.2709\n",
            "Updating learning rate to 4.0026907125422175e-05\n",
            "epoch: 59 MSE loss: 0.1185 MSE valid loss: 0.2708\n",
            "Updating learning rate to 3.969754537023144e-05\n",
            "epoch: 60 MSE loss: 0.1191 MSE valid loss: 0.2661\n",
            "Updating learning rate to 3.937618253373847e-05\n",
            "epoch: 61 MSE loss: 0.1179 MSE valid loss: 0.2747\n",
            "Updating learning rate to 3.90625e-05\n",
            "epoch: 62 MSE loss: 0.1177 MSE valid loss: 0.2726\n",
            "Updating learning rate to 3.875619664232189e-05\n",
            "epoch: 63 MSE loss: 0.1180 MSE valid loss: 0.2698\n",
            "Updating learning rate to 3.845698760800996e-05\n",
            "epoch: 64 MSE loss: 0.1184 MSE valid loss: 0.2678\n",
            "Updating learning rate to 3.816460320475954e-05\n",
            "epoch: 65 MSE loss: 0.1173 MSE valid loss: 0.2682\n",
            "Updating learning rate to 3.787878787878788e-05\n",
            "epoch: 66 MSE loss: 0.1172 MSE valid loss: 0.2749\n",
            "Updating learning rate to 3.759929927590882e-05\n",
            "epoch: 67 MSE loss: 0.1175 MSE valid loss: 0.2699\n",
            "Updating learning rate to 3.732590737770921e-05\n",
            "epoch: 68 MSE loss: 0.1171 MSE valid loss: 0.2650\n",
            "Updating learning rate to 3.7058393705829335e-05\n",
            "epoch: 69 MSE loss: 0.1171 MSE valid loss: 0.2623\n",
            "Updating learning rate to 3.679655058809149e-05\n",
            "epoch: 70 MSE loss: 0.1169 MSE valid loss: 0.2681\n",
            "Updating learning rate to 3.654018048087443e-05\n",
            "epoch: 71 MSE loss: 0.1168 MSE valid loss: 0.2653\n",
            "Updating learning rate to 3.6289095342709304e-05\n",
            "epoch: 72 MSE loss: 0.1162 MSE valid loss: 0.2721\n",
            "Updating learning rate to 3.604311605458291e-05\n",
            "epoch: 73 MSE loss: 0.1158 MSE valid loss: 0.2708\n",
            "Updating learning rate to 3.58020718828878e-05\n",
            "epoch: 74 MSE loss: 0.1161 MSE valid loss: 0.2693\n",
            "Updating learning rate to 3.5565799981359994e-05\n",
            "epoch: 75 MSE loss: 0.1162 MSE valid loss: 0.2702\n",
            "Updating learning rate to 3.5334144928703015e-05\n",
            "epoch: 76 MSE loss: 0.1172 MSE valid loss: 0.2716\n",
            "Updating learning rate to 3.510695829891526e-05\n",
            "epoch: 77 MSE loss: 0.1153 MSE valid loss: 0.2757\n",
            "Updating learning rate to 3.488409826162172e-05\n",
            "epoch: 78 MSE loss: 0.1151 MSE valid loss: 0.2695\n",
            "Updating learning rate to 3.4665429209964995e-05\n",
            "epoch: 79 MSE loss: 0.1147 MSE valid loss: 0.2770\n",
            "Updating learning rate to 3.445082141383733e-05\n",
            "epoch: 80 MSE loss: 0.1147 MSE valid loss: 0.2688\n",
            "Updating learning rate to 3.424015069643934e-05\n",
            "epoch: 81 MSE loss: 0.1141 MSE valid loss: 0.2735\n",
            "Updating learning rate to 3.4033298132333136e-05\n",
            "epoch: 82 MSE loss: 0.1146 MSE valid loss: 0.2780\n",
            "Updating learning rate to 3.383014976532195e-05\n",
            "epoch: 83 MSE loss: 0.1150 MSE valid loss: 0.2753\n",
            "Updating learning rate to 3.3630596344635906e-05\n",
            "epoch: 84 MSE loss: 0.1149 MSE valid loss: 0.2762\n",
            "Updating learning rate to 3.343453307803635e-05\n",
            "epoch: 85 MSE loss: 0.1134 MSE valid loss: 0.2756\n",
            "Updating learning rate to 3.3241859400571366e-05\n",
            "epoch: 86 MSE loss: 0.1137 MSE valid loss: 0.2746\n",
            "Updating learning rate to 3.305247875782325e-05\n",
            "epoch: 87 MSE loss: 0.1144 MSE valid loss: 0.2812\n",
            "Updating learning rate to 3.286629840258662e-05\n",
            "epoch: 88 MSE loss: 0.1148 MSE valid loss: 0.2735\n",
            "Updating learning rate to 3.268322920400467e-05\n",
            "epoch: 89 MSE loss: 0.1131 MSE valid loss: 0.2746\n",
            "Updating learning rate to 3.250318546827149e-05\n",
            "epoch: 90 MSE loss: 0.1141 MSE valid loss: 0.2783\n",
            "Updating learning rate to 3.232608477008077e-05\n",
            "epoch: 91 MSE loss: 0.1126 MSE valid loss: 0.2790\n",
            "Updating learning rate to 3.21518477940684e-05\n",
            "epoch: 92 MSE loss: 0.1134 MSE valid loss: 0.2841\n",
            "Updating learning rate to 3.198039818555568e-05\n",
            "epoch: 93 MSE loss: 0.1126 MSE valid loss: 0.2725\n",
            "Updating learning rate to 3.181166240995547e-05\n",
            "epoch: 94 MSE loss: 0.1129 MSE valid loss: 0.2865\n",
            "Updating learning rate to 3.1645569620253167e-05\n",
            "epoch: 95 MSE loss: 0.1131 MSE valid loss: 0.2846\n",
            "Updating learning rate to 3.148205153202001e-05\n",
            "epoch: 96 MSE loss: 0.1124 MSE valid loss: 0.2752\n",
            "Updating learning rate to 3.1321042305458135e-05\n",
            "epoch: 97 MSE loss: 0.1130 MSE valid loss: 0.2875\n",
            "Updating learning rate to 3.1162478434014266e-05\n",
            "epoch: 98 MSE loss: 0.1124 MSE valid loss: 0.2844\n",
            "Updating learning rate to 3.100629863913435e-05\n",
            "epoch: 99 MSE loss: 0.1127 MSE valid loss: 0.2876\n",
            "Updating learning rate to 4.811252243246882e-08\n",
            "Updating learning rate to 3.175426480542942e-06\n",
            "epoch: 0 MSE loss: 0.2394 MSE valid loss: 0.1333\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 6.302740438653415e-06\n",
            "epoch: 1 MSE loss: 0.2356 MSE valid loss: 0.1289\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.430054396763887e-06\n",
            "epoch: 2 MSE loss: 0.2266 MSE valid loss: 0.1227\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 1.2557368354874362e-05\n",
            "epoch: 3 MSE loss: 0.2153 MSE valid loss: 0.1170\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 1.3846219390542782e-05\n",
            "epoch: 4 MSE loss: 0.2065 MSE valid loss: 0.1122\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 1.264304343560434e-05\n",
            "epoch: 5 MSE loss: 0.1977 MSE valid loss: 0.1087\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 1.1707322644771175e-05\n",
            "epoch: 6 MSE loss: 0.1893 MSE valid loss: 0.1064\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 1.0952698858458087e-05\n",
            "epoch: 7 MSE loss: 0.1831 MSE valid loss: 0.1053\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 1.0327404809650346e-05\n",
            "epoch: 8 MSE loss: 0.1781 MSE valid loss: 0.1046\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.798272520870257e-06\n",
            "epoch: 9 MSE loss: 0.1736 MSE valid loss: 0.1044\n",
            "Best score! Weights of the model are updated!\n",
            "Updating learning rate to 9.342938659399199e-06\n",
            "epoch: 10 MSE loss: 0.1706 MSE valid loss: 0.1047\n",
            "Updating learning rate to 8.945703337056415e-06\n",
            "epoch: 11 MSE loss: 0.1678 MSE valid loss: 0.1051\n",
            "Updating learning rate to 8.595177052156612e-06\n",
            "epoch: 12 MSE loss: 0.1636 MSE valid loss: 0.1054\n",
            "Updating learning rate to 8.282869524032146e-06\n",
            "epoch: 13 MSE loss: 0.1633 MSE valid loss: 0.1057\n",
            "Updating learning rate to 8.002304995806e-06\n",
            "epoch: 14 MSE loss: 0.1599 MSE valid loss: 0.1062\n",
            "Updating learning rate to 7.748446592171797e-06\n",
            "epoch: 15 MSE loss: 0.1573 MSE valid loss: 0.1069\n",
            "Updating learning rate to 7.517309741553296e-06\n",
            "epoch: 16 MSE loss: 0.1551 MSE valid loss: 0.1076\n",
            "Updating learning rate to 7.305695402335044e-06\n",
            "epoch: 17 MSE loss: 0.1543 MSE valid loss: 0.1083\n",
            "Updating learning rate to 7.1110015498571785e-06\n",
            "epoch: 18 MSE loss: 0.1528 MSE valid loss: 0.1090\n",
            "Updating learning rate to 6.931087162517846e-06\n",
            "epoch: 19 MSE loss: 0.1517 MSE valid loss: 0.1098\n",
            "Updating learning rate to 6.76417225936176e-06\n",
            "epoch: 20 MSE loss: 0.1502 MSE valid loss: 0.1102\n",
            "Updating learning rate to 6.608763214317868e-06\n",
            "epoch: 21 MSE loss: 0.1489 MSE valid loss: 0.1107\n",
            "Updating learning rate to 6.463596124937739e-06\n",
            "epoch: 22 MSE loss: 0.1478 MSE valid loss: 0.1114\n",
            "Updating learning rate to 6.327593294406922e-06\n",
            "epoch: 23 MSE loss: 0.1468 MSE valid loss: 0.1122\n",
            "Updating learning rate to 6.199829383043036e-06\n",
            "epoch: 24 MSE loss: 0.1465 MSE valid loss: 0.1126\n",
            "Updating learning rate to 6.079504788572469e-06\n",
            "epoch: 25 MSE loss: 0.1450 MSE valid loss: 0.1131\n",
            "Updating learning rate to 5.965924498791846e-06\n",
            "epoch: 26 MSE loss: 0.1443 MSE valid loss: 0.1139\n",
            "Updating learning rate to 5.8584811349126495e-06\n",
            "epoch: 27 MSE loss: 0.1442 MSE valid loss: 0.1147\n",
            "Updating learning rate to 5.7566412382313e-06\n",
            "epoch: 28 MSE loss: 0.1431 MSE valid loss: 0.1152\n",
            "Updating learning rate to 5.659934091583234e-06\n",
            "epoch: 29 MSE loss: 0.1422 MSE valid loss: 0.1157\n",
            "Updating learning rate to 5.567942539842175e-06\n",
            "epoch: 30 MSE loss: 0.1413 MSE valid loss: 0.1162\n",
            "Updating learning rate to 5.48029540026768e-06\n",
            "epoch: 31 MSE loss: 0.1411 MSE valid loss: 0.1168\n",
            "Updating learning rate to 5.39666114720432e-06\n",
            "epoch: 32 MSE loss: 0.1403 MSE valid loss: 0.1168\n",
            "Updating learning rate to 5.316742625738919e-06\n",
            "epoch: 33 MSE loss: 0.1401 MSE valid loss: 0.1172\n",
            "Updating learning rate to 5.240272601878983e-06\n",
            "epoch: 34 MSE loss: 0.1396 MSE valid loss: 0.1174\n",
            "Updating learning rate to 5.16700999718195e-06\n",
            "epoch: 35 MSE loss: 0.1389 MSE valid loss: 0.1180\n",
            "Updating learning rate to 5.096736686795811e-06\n",
            "epoch: 36 MSE loss: 0.1381 MSE valid loss: 0.1185\n",
            "Updating learning rate to 5.029254763916053e-06\n",
            "epoch: 37 MSE loss: 0.1387 MSE valid loss: 0.1191\n",
            "Updating learning rate to 4.964384192434611e-06\n",
            "epoch: 38 MSE loss: 0.1376 MSE valid loss: 0.1190\n",
            "Updating learning rate to 4.901960784313726e-06\n",
            "epoch: 39 MSE loss: 0.1366 MSE valid loss: 0.1196\n",
            "Updating learning rate to 4.841834449897069e-06\n",
            "epoch: 40 MSE loss: 0.1363 MSE valid loss: 0.1196\n",
            "Updating learning rate to 4.783867678672252e-06\n",
            "epoch: 41 MSE loss: 0.1358 MSE valid loss: 0.1204\n",
            "Updating learning rate to 4.727934215451461e-06\n",
            "epoch: 42 MSE loss: 0.1355 MSE valid loss: 0.1204\n",
            "Updating learning rate to 4.673917902941745e-06\n",
            "epoch: 43 MSE loss: 0.1356 MSE valid loss: 0.1207\n",
            "Updating learning rate to 4.621711666540848e-06\n",
            "epoch: 44 MSE loss: 0.1350 MSE valid loss: 0.1207\n",
            "Updating learning rate to 4.571216621155238e-06\n",
            "epoch: 45 MSE loss: 0.1352 MSE valid loss: 0.1217\n",
            "Updating learning rate to 4.5223412830776355e-06\n",
            "epoch: 46 MSE loss: 0.1343 MSE valid loss: 0.1212\n",
            "Updating learning rate to 4.475000872625255e-06\n",
            "epoch: 47 MSE loss: 0.1331 MSE valid loss: 0.1220\n",
            "Updating learning rate to 4.42911669543945e-06\n",
            "epoch: 48 MSE loss: 0.1329 MSE valid loss: 0.1223\n",
            "Updating learning rate to 4.3846155921711576e-06\n",
            "epoch: 49 MSE loss: 0.1321 MSE valid loss: 0.1219\n",
            "Updating learning rate to 4.341429447794925e-06\n",
            "epoch: 50 MSE loss: 0.1316 MSE valid loss: 0.1222\n",
            "Updating learning rate to 4.299494753063186e-06\n",
            "epoch: 51 MSE loss: 0.1318 MSE valid loss: 0.1224\n",
            "Updating learning rate to 4.258752211677067e-06\n",
            "epoch: 52 MSE loss: 0.1318 MSE valid loss: 0.1226\n",
            "Updating learning rate to 4.219146387646126e-06\n",
            "epoch: 53 MSE loss: 0.1313 MSE valid loss: 0.1225\n",
            "Updating learning rate to 4.18062538806657e-06\n",
            "epoch: 54 MSE loss: 0.1316 MSE valid loss: 0.1230\n",
            "Updating learning rate to 4.143140577189122e-06\n",
            "epoch: 55 MSE loss: 0.1304 MSE valid loss: 0.1232\n",
            "Updating learning rate to 4.106646318193315e-06\n",
            "epoch: 56 MSE loss: 0.1301 MSE valid loss: 0.1232\n",
            "Updating learning rate to 4.071099739550263e-06\n",
            "epoch: 57 MSE loss: 0.1294 MSE valid loss: 0.1230\n",
            "Updating learning rate to 4.036460523253916e-06\n",
            "epoch: 58 MSE loss: 0.1290 MSE valid loss: 0.1228\n",
            "Updating learning rate to 4.002690712542218e-06\n",
            "epoch: 59 MSE loss: 0.1291 MSE valid loss: 0.1234\n",
            "Updating learning rate to 3.969754537023144e-06\n",
            "epoch: 60 MSE loss: 0.1291 MSE valid loss: 0.1232\n",
            "Updating learning rate to 3.937618253373847e-06\n",
            "epoch: 61 MSE loss: 0.1287 MSE valid loss: 0.1236\n",
            "Updating learning rate to 3.90625e-06\n",
            "epoch: 62 MSE loss: 0.1281 MSE valid loss: 0.1235\n",
            "Updating learning rate to 3.8756196642321895e-06\n",
            "epoch: 63 MSE loss: 0.1273 MSE valid loss: 0.1238\n",
            "Updating learning rate to 3.845698760800996e-06\n",
            "epoch: 64 MSE loss: 0.1277 MSE valid loss: 0.1237\n",
            "Updating learning rate to 3.816460320475954e-06\n",
            "epoch: 65 MSE loss: 0.1272 MSE valid loss: 0.1242\n",
            "Updating learning rate to 3.7878787878787882e-06\n",
            "epoch: 66 MSE loss: 0.1267 MSE valid loss: 0.1245\n",
            "Updating learning rate to 3.759929927590882e-06\n",
            "epoch: 67 MSE loss: 0.1265 MSE valid loss: 0.1242\n",
            "Updating learning rate to 3.732590737770921e-06\n",
            "epoch: 68 MSE loss: 0.1265 MSE valid loss: 0.1244\n",
            "Updating learning rate to 3.7058393705829336e-06\n",
            "epoch: 69 MSE loss: 0.1263 MSE valid loss: 0.1242\n",
            "Updating learning rate to 3.6796550588091485e-06\n",
            "epoch: 70 MSE loss: 0.1256 MSE valid loss: 0.1250\n",
            "Updating learning rate to 3.6540180480874438e-06\n",
            "epoch: 71 MSE loss: 0.1257 MSE valid loss: 0.1243\n",
            "Updating learning rate to 3.6289095342709304e-06\n",
            "epoch: 72 MSE loss: 0.1254 MSE valid loss: 0.1244\n",
            "Updating learning rate to 3.604311605458291e-06\n",
            "epoch: 73 MSE loss: 0.1260 MSE valid loss: 0.1247\n",
            "Updating learning rate to 3.58020718828878e-06\n",
            "epoch: 74 MSE loss: 0.1241 MSE valid loss: 0.1251\n",
            "Updating learning rate to 3.5565799981359997e-06\n",
            "epoch: 75 MSE loss: 0.1243 MSE valid loss: 0.1247\n",
            "Updating learning rate to 3.5334144928703013e-06\n",
            "epoch: 76 MSE loss: 0.1249 MSE valid loss: 0.1248\n",
            "Updating learning rate to 3.5106958298915256e-06\n",
            "epoch: 77 MSE loss: 0.1237 MSE valid loss: 0.1252\n",
            "Updating learning rate to 3.4884098261621723e-06\n",
            "epoch: 78 MSE loss: 0.1239 MSE valid loss: 0.1253\n",
            "Updating learning rate to 3.466542920996499e-06\n",
            "epoch: 79 MSE loss: 0.1235 MSE valid loss: 0.1249\n",
            "Updating learning rate to 3.4450821413837328e-06\n",
            "epoch: 80 MSE loss: 0.1227 MSE valid loss: 0.1250\n",
            "Updating learning rate to 3.4240150696439336e-06\n",
            "epoch: 81 MSE loss: 0.1231 MSE valid loss: 0.1253\n",
            "Updating learning rate to 3.4033298132333132e-06\n",
            "epoch: 82 MSE loss: 0.1228 MSE valid loss: 0.1253\n",
            "Updating learning rate to 3.383014976532195e-06\n",
            "epoch: 83 MSE loss: 0.1231 MSE valid loss: 0.1254\n",
            "Updating learning rate to 3.3630596344635904e-06\n",
            "epoch: 84 MSE loss: 0.1226 MSE valid loss: 0.1258\n",
            "Updating learning rate to 3.3434533078036348e-06\n",
            "epoch: 85 MSE loss: 0.1225 MSE valid loss: 0.1257\n",
            "Updating learning rate to 3.3241859400571367e-06\n",
            "epoch: 86 MSE loss: 0.1215 MSE valid loss: 0.1260\n",
            "Updating learning rate to 3.305247875782325e-06\n",
            "epoch: 87 MSE loss: 0.1224 MSE valid loss: 0.1262\n",
            "Updating learning rate to 3.2866298402586612e-06\n",
            "epoch: 88 MSE loss: 0.1213 MSE valid loss: 0.1263\n",
            "Updating learning rate to 3.2683229204004673e-06\n",
            "epoch: 89 MSE loss: 0.1220 MSE valid loss: 0.1261\n",
            "Updating learning rate to 3.2503185468271486e-06\n",
            "epoch: 90 MSE loss: 0.1208 MSE valid loss: 0.1261\n",
            "Updating learning rate to 3.232608477008077e-06\n",
            "epoch: 91 MSE loss: 0.1218 MSE valid loss: 0.1263\n",
            "Updating learning rate to 3.2151847794068398e-06\n",
            "epoch: 92 MSE loss: 0.1213 MSE valid loss: 0.1269\n",
            "Updating learning rate to 3.1980398185555676e-06\n",
            "epoch: 93 MSE loss: 0.1210 MSE valid loss: 0.1265\n",
            "Updating learning rate to 3.1811662409955475e-06\n",
            "epoch: 94 MSE loss: 0.1205 MSE valid loss: 0.1266\n",
            "Updating learning rate to 3.1645569620253167e-06\n",
            "epoch: 95 MSE loss: 0.1201 MSE valid loss: 0.1266\n",
            "Updating learning rate to 3.1482051532020014e-06\n",
            "epoch: 96 MSE loss: 0.1196 MSE valid loss: 0.1268\n",
            "Updating learning rate to 3.1321042305458136e-06\n",
            "epoch: 97 MSE loss: 0.1203 MSE valid loss: 0.1269\n",
            "Updating learning rate to 3.116247843401426e-06\n",
            "epoch: 98 MSE loss: 0.1199 MSE valid loss: 0.1269\n",
            "Updating learning rate to 3.1006298639134353e-06\n",
            "epoch: 99 MSE loss: 0.1197 MSE valid loss: 0.1269\n"
          ]
        }
      ],
      "source": [
        "Linear_train_history, Linear_valid_history = Linear_learner.train()\n",
        "DLinear_train_history, DLinear_valid_history = DLinear_learner.train()\n",
        "patchtst_train_history, patchtst_valid_history = patchtst_learner.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distillation_learner = DistillationLearner(\n",
        "    teacher_model=teacher_model,\n",
        "    student_model=student_model,\n",
        "    dataset=ETTDataset,\n",
        "    batch_size=128,\n",
        "    lr=0.001,\n",
        "    epochs=100,\n",
        "    alpha=0.5,  # 증류 손실 비율 변경\n",
        "    temperature=2.5  # 로짓 부드러움 조정\n",
        ")\n",
        "\n",
        "student_train_history, student_valid_history = distillation_learner.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiPPVfwErWcb",
        "outputId": "9fd2b689-7b34-41f4-f24e-2300cba41f3c"
      },
      "id": "JiPPVfwErWcb",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 MSE loss: 0.4557 MSE valid loss: 0.3547\n",
            "epoch: 2 MSE loss: 0.4432 MSE valid loss: 0.3455\n",
            "epoch: 3 MSE loss: 0.4177 MSE valid loss: 0.3320\n",
            "epoch: 4 MSE loss: 0.3740 MSE valid loss: 0.3145\n",
            "epoch: 5 MSE loss: 0.3188 MSE valid loss: 0.2948\n",
            "epoch: 6 MSE loss: 0.2536 MSE valid loss: 0.2780\n",
            "epoch: 7 MSE loss: 0.2053 MSE valid loss: 0.2652\n",
            "epoch: 8 MSE loss: 0.1707 MSE valid loss: 0.2571\n",
            "epoch: 9 MSE loss: 0.1451 MSE valid loss: 0.2562\n",
            "epoch: 10 MSE loss: 0.1290 MSE valid loss: 0.2569\n",
            "epoch: 11 MSE loss: 0.1182 MSE valid loss: 0.2565\n",
            "epoch: 12 MSE loss: 0.1103 MSE valid loss: 0.2539\n",
            "epoch: 13 MSE loss: 0.1054 MSE valid loss: 0.2471\n",
            "epoch: 14 MSE loss: 0.1017 MSE valid loss: 0.2373\n",
            "epoch: 15 MSE loss: 0.0983 MSE valid loss: 0.2264\n",
            "epoch: 16 MSE loss: 0.0956 MSE valid loss: 0.2146\n",
            "epoch: 17 MSE loss: 0.0940 MSE valid loss: 0.2037\n",
            "epoch: 18 MSE loss: 0.0922 MSE valid loss: 0.1945\n",
            "epoch: 19 MSE loss: 0.0902 MSE valid loss: 0.1883\n",
            "epoch: 20 MSE loss: 0.0887 MSE valid loss: 0.1813\n",
            "epoch: 21 MSE loss: 0.0877 MSE valid loss: 0.1764\n",
            "epoch: 22 MSE loss: 0.0868 MSE valid loss: 0.1726\n",
            "epoch: 23 MSE loss: 0.0859 MSE valid loss: 0.1698\n",
            "epoch: 24 MSE loss: 0.0856 MSE valid loss: 0.1675\n",
            "epoch: 25 MSE loss: 0.0850 MSE valid loss: 0.1655\n",
            "epoch: 26 MSE loss: 0.0838 MSE valid loss: 0.1646\n",
            "epoch: 27 MSE loss: 0.0837 MSE valid loss: 0.1632\n",
            "epoch: 28 MSE loss: 0.0831 MSE valid loss: 0.1618\n",
            "epoch: 29 MSE loss: 0.0834 MSE valid loss: 0.1618\n",
            "epoch: 30 MSE loss: 0.0822 MSE valid loss: 0.1614\n",
            "epoch: 31 MSE loss: 0.0817 MSE valid loss: 0.1605\n",
            "epoch: 32 MSE loss: 0.0813 MSE valid loss: 0.1602\n",
            "epoch: 33 MSE loss: 0.0809 MSE valid loss: 0.1596\n",
            "epoch: 34 MSE loss: 0.0807 MSE valid loss: 0.1592\n",
            "epoch: 35 MSE loss: 0.0801 MSE valid loss: 0.1587\n",
            "epoch: 36 MSE loss: 0.0799 MSE valid loss: 0.1583\n",
            "epoch: 37 MSE loss: 0.0794 MSE valid loss: 0.1580\n",
            "epoch: 38 MSE loss: 0.0793 MSE valid loss: 0.1579\n",
            "epoch: 39 MSE loss: 0.0795 MSE valid loss: 0.1574\n",
            "epoch: 40 MSE loss: 0.0784 MSE valid loss: 0.1576\n",
            "epoch: 41 MSE loss: 0.0781 MSE valid loss: 0.1576\n",
            "epoch: 42 MSE loss: 0.0779 MSE valid loss: 0.1572\n",
            "epoch: 43 MSE loss: 0.0776 MSE valid loss: 0.1574\n",
            "epoch: 44 MSE loss: 0.0773 MSE valid loss: 0.1571\n",
            "epoch: 45 MSE loss: 0.0768 MSE valid loss: 0.1569\n",
            "epoch: 46 MSE loss: 0.0766 MSE valid loss: 0.1571\n",
            "epoch: 47 MSE loss: 0.0767 MSE valid loss: 0.1569\n",
            "epoch: 48 MSE loss: 0.0761 MSE valid loss: 0.1573\n",
            "epoch: 49 MSE loss: 0.0757 MSE valid loss: 0.1571\n",
            "epoch: 50 MSE loss: 0.0757 MSE valid loss: 0.1571\n",
            "epoch: 51 MSE loss: 0.0758 MSE valid loss: 0.1574\n",
            "epoch: 52 MSE loss: 0.0750 MSE valid loss: 0.1574\n",
            "epoch: 53 MSE loss: 0.0748 MSE valid loss: 0.1575\n",
            "epoch: 54 MSE loss: 0.0747 MSE valid loss: 0.1578\n",
            "epoch: 55 MSE loss: 0.0741 MSE valid loss: 0.1579\n",
            "epoch: 56 MSE loss: 0.0739 MSE valid loss: 0.1583\n",
            "epoch: 57 MSE loss: 0.0737 MSE valid loss: 0.1583\n",
            "epoch: 58 MSE loss: 0.0739 MSE valid loss: 0.1586\n",
            "epoch: 59 MSE loss: 0.0736 MSE valid loss: 0.1589\n",
            "epoch: 60 MSE loss: 0.0734 MSE valid loss: 0.1592\n",
            "epoch: 61 MSE loss: 0.0730 MSE valid loss: 0.1595\n",
            "epoch: 62 MSE loss: 0.0726 MSE valid loss: 0.1596\n",
            "epoch: 63 MSE loss: 0.0725 MSE valid loss: 0.1599\n",
            "epoch: 64 MSE loss: 0.0723 MSE valid loss: 0.1605\n",
            "epoch: 65 MSE loss: 0.0723 MSE valid loss: 0.1605\n",
            "epoch: 66 MSE loss: 0.0722 MSE valid loss: 0.1610\n",
            "epoch: 67 MSE loss: 0.0721 MSE valid loss: 0.1612\n",
            "epoch: 68 MSE loss: 0.0718 MSE valid loss: 0.1613\n",
            "epoch: 69 MSE loss: 0.0712 MSE valid loss: 0.1615\n",
            "epoch: 70 MSE loss: 0.0711 MSE valid loss: 0.1621\n",
            "epoch: 71 MSE loss: 0.0711 MSE valid loss: 0.1623\n",
            "epoch: 72 MSE loss: 0.0709 MSE valid loss: 0.1623\n",
            "epoch: 73 MSE loss: 0.0705 MSE valid loss: 0.1622\n",
            "epoch: 74 MSE loss: 0.0704 MSE valid loss: 0.1626\n",
            "epoch: 75 MSE loss: 0.0701 MSE valid loss: 0.1629\n",
            "epoch: 76 MSE loss: 0.0700 MSE valid loss: 0.1629\n",
            "epoch: 77 MSE loss: 0.0699 MSE valid loss: 0.1629\n",
            "epoch: 78 MSE loss: 0.0699 MSE valid loss: 0.1634\n",
            "epoch: 79 MSE loss: 0.0697 MSE valid loss: 0.1633\n",
            "epoch: 80 MSE loss: 0.0694 MSE valid loss: 0.1638\n",
            "epoch: 81 MSE loss: 0.0692 MSE valid loss: 0.1638\n",
            "epoch: 82 MSE loss: 0.0689 MSE valid loss: 0.1640\n",
            "epoch: 83 MSE loss: 0.0689 MSE valid loss: 0.1639\n",
            "epoch: 84 MSE loss: 0.0687 MSE valid loss: 0.1645\n",
            "epoch: 85 MSE loss: 0.0689 MSE valid loss: 0.1643\n",
            "epoch: 86 MSE loss: 0.0687 MSE valid loss: 0.1646\n",
            "epoch: 87 MSE loss: 0.0682 MSE valid loss: 0.1643\n",
            "epoch: 88 MSE loss: 0.0688 MSE valid loss: 0.1645\n",
            "epoch: 89 MSE loss: 0.0682 MSE valid loss: 0.1647\n",
            "epoch: 90 MSE loss: 0.0679 MSE valid loss: 0.1647\n",
            "epoch: 91 MSE loss: 0.0676 MSE valid loss: 0.1647\n",
            "epoch: 92 MSE loss: 0.0679 MSE valid loss: 0.1653\n",
            "epoch: 93 MSE loss: 0.0673 MSE valid loss: 0.1650\n",
            "epoch: 94 MSE loss: 0.0673 MSE valid loss: 0.1649\n",
            "epoch: 95 MSE loss: 0.0672 MSE valid loss: 0.1649\n",
            "epoch: 96 MSE loss: 0.0670 MSE valid loss: 0.1652\n",
            "epoch: 97 MSE loss: 0.0668 MSE valid loss: 0.1651\n",
            "epoch: 98 MSE loss: 0.0665 MSE valid loss: 0.1654\n",
            "epoch: 99 MSE loss: 0.0666 MSE valid loss: 0.1657\n",
            "epoch: 100 MSE loss: 0.0663 MSE valid loss: 0.1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cfb1124f-d3bc-4fa3-ad4b-f7a7a6105ecb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfb1124f-d3bc-4fa3-ad4b-f7a7a6105ecb",
        "outputId": "b8a87ce5-bf50-4f4a-b155-702f0a6f77d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE test loss: 0.0964\n"
          ]
        }
      ],
      "source": [
        "Linear_learner.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "29d937b8-f16a-4772-923b-81b6697fe18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29d937b8-f16a-4772-923b-81b6697fe18e",
        "outputId": "53472902-e6b4-490e-eb03-c265b2d283b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE test loss: 0.1098\n"
          ]
        }
      ],
      "source": [
        "DLinear_learner.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "608cbcf6-ed91-410e-973f-5eea8748d813",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "608cbcf6-ed91-410e-973f-5eea8748d813",
        "outputId": "1e9013c2-c455-4fca-b311-a0ddc7973fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE test loss: 0.0632\n"
          ]
        }
      ],
      "source": [
        "patchtst_learner.test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distillation_learner.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4dbN6s93ixr",
        "outputId": "f861bdd3-1bb8-4b95-cdee-d63326df0c97"
      },
      "id": "d4dbN6s93ixr",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE test loss: 0.3857\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38571996851400897"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distillation_learner.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1WAdwyKQvhp",
        "outputId": "558c492e-021b-4e96-edec-d8d44d03fb10"
      },
      "id": "C1WAdwyKQvhp",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE test loss: 0.3857\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38571996851400897"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pjGdPSKLQxVg"
      },
      "id": "pjGdPSKLQxVg",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "041b8b9d-8fde-4a67-93e6-d7d25139bb44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "041b8b9d-8fde-4a67-93e6-d7d25139bb44",
        "outputId": "4b6b4657-f59a-404c-f234-46ae08d7ee22"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# === 수정: 결과 시각화 ===\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "ax.plot(np.arange(1, Linear_learner.epochs+1), Linear_valid_history, label=\"Linear\")\n",
        "ax.plot(np.arange(1, DLinear_learner.epochs+1), DLinear_valid_history, label=\"DLinear\")\n",
        "ax.plot(np.arange(1, patchtst_learner.epochs+1), patchtst_valid_history, label=\"PatchTST (Teacher)\")\n",
        "ax.plot(np.arange(1, student_learner.epochs+1), student_valid_history, label=\"Student Model\")  # 추가\n",
        "ax.set_xlabel(\"epochs\")\n",
        "ax.set_ylabel(\"MSE Error\")\n",
        "ax.set_ylim(0, 1.5)\n",
        "ax.legend()\n",
        "plt.title(\"Validation Error Comparison\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c951c62f-3c57-4277-8d84-e5c9072899d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c951c62f-3c57-4277-8d84-e5c9072899d7",
        "outputId": "758124b7-3827-45e6-9925-cc4b58351b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model Parameters: 468800\n",
            "Student Model Parameters: 104848\n",
            "Compression Ratio: 4.47x\n",
            "Teacher Model Size: 1861.20 KB\n",
            "Student Model Size: 420.38 KB\n"
          ]
        }
      ],
      "source": [
        "teacher_params = count_parameters(patchtst_model)\n",
        "student_params = count_parameters(student_model)\n",
        "\n",
        "print(f\"Teacher Model Parameters: {teacher_params}\")\n",
        "print(f\"Student Model Parameters: {student_params}\")\n",
        "print(f\"Compression Ratio: {teacher_params / student_params:.2f}x\")\n",
        "\n",
        "torch.save(patchtst_model.state_dict(), \"teacher_model.pth\")\n",
        "torch.save(student_model.state_dict(), \"student_model.pth\")\n",
        "\n",
        "import os\n",
        "teacher_size = os.path.getsize(\"teacher_model.pth\") / 1024  # KB\n",
        "student_size = os.path.getsize(\"student_model.pth\") / 1024  # KB\n",
        "\n",
        "print(f\"Teacher Model Size: {teacher_size:.2f} KB\")\n",
        "print(f\"Student Model Size: {student_size:.2f} KB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}